#-----å¯¼å…¥åº“
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd 
import seaborn as sns
from sklearn import metrics
from pandas import read_csv
from matplotlib import pyplot
from sqlalchemy import values

from numpy import asarray, quantile
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.ensemble import RandomForestRegressor


#-----å¯¼å…¥æ•°æ®
series = read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv', header = 0, index_col = 0)
print(series)
values = series.values

#-----å¯è§†åŒ–æ•°æ®
pyplot.plot(values)
pyplot.show()

#-----åˆ›å»ºå‡½æ•°æ¥å¤„ç†æ•°æ®å¹¶å‚¨å­˜

def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    n_vars = 1 if type(data) is list else  data.shape[1]
    df = DataFrame(data)
    cols = list()

    for i in range(n_in,0,-1):
        cols.append(df.shift(i))

    for i in range(0,n_out):
        cols.append(df.shift(-i))
    
    agg = concat(cols,axis=1)

    if dropnan:
        agg.dropna(inplace=True)

    return agg.values

series = read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-max-temperatures.csv', header=0, index_col=0)
data = series.values

#------é¢„æµ‹
n_test = 100 #æ¨¡å‹æµ‹è¯•çš„æ•°é‡æ˜¯100ä¸ª

def train_test_split(data,n_test): #æŠŠæ‰€æœ‰æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    return data[:-n_test,:],data[-n_test:,:]

train,test = train_test_split(data,n_test) # train_test_splitï¼ˆï¼‰å‡½æ•°å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†trainå’Œæµ‹è¯•é›†testï¼Œæ•°æ®åº“ä¸ºdataï¼Œéšæœºæ•°é‡ä¸º100

#--è®­ç»ƒé›†
Train = series_to_supervised(train,n_in = 6) #å°†æ—¶é—´æ•°æ®é›†è½¬æ¢ä¸ºç”¨äºå­¦ä¹ è®­ç»ƒçš„æ•°æ®é›†,ä»ä¸Šé¢ğŸ‘†åˆ’åˆ†å‡ºæ¥çš„trainä¸­é€‰60%
X_train,y_train = Train[:,:-1],Train[:,-1] # X:è¦åˆ’åˆ†çš„æ ·æœ¬ç‰¹å¾é›†ï¼ˆè¾“å…¥çš„ä¿¡æ¯ï¼‰ y:éœ€è¦åˆ’åˆ†çš„æ ·æœ¬ç»“æœï¼ˆè¾“å‡ºç»“æœï¼‰
print("--Train-- :",Train)

##æ¢¯åº¦æå‡----------------------------------------

#--æµ‹è¯•é›†
Test = series_to_supervised(train,n_in = 6)
X_test,y_test = Train[:,:-1],Train[:,-1]
print("--Test--: ",Test)

#-----å»ºç«‹å›å½’æ¨¡å‹ï¼Œä½¿ç”¨è®­ç»ƒé›†train
model = RandomForestRegressor(n_estimators = 1000) #n_estimatorsï¼šå†³ç­–æ ‘çš„ä¸ªæ•°ï¼Œè¶Šå¤šè¶Šå¥½ï¼Œä½†æ˜¯æ€§èƒ½å°±ä¼šè¶Šå·®
model.fit(X_train,y_train)

#-----é¢„æµ‹
row = values[-6:].flatten() # ä¸Šé¢n_inæ˜¯6ï¼Œæ‰€ä»¥å€’åºæ’åˆ—ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ ä¸º-6ï¼Œjå€¼ç¼ºçœé»˜è®¤æ˜¯0ï¼Œ
                            # å…·ä½“è§https://blog.csdn.net/qiushangren/article/details/103550923?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_ecpm_v1~rank_v31_ecpm-4-103550923-null-null.pc_agg_new_rank&utm_term=Pythonä¸­values%E3%80%94%3A%2C0%E3%80%95ä»€ä¹ˆæ„æ€&spm=1000.2123.3001.4430 ,
                            # åœ¨æœ«å°¾åŠ ä¸€ä¸ªflatten() å˜æˆä¸€è¡Œçš„æ–¹ä¾¿ç»Ÿè®¡åˆ†æ,é»˜è®¤ç¼ºçœå‚æ•°ä¸º0ï¼Œä¹Ÿå°±æ˜¯è¯´flatten()å’Œflatte(0)æ•ˆæœä¸€æ ·ã€‚
yhat = model.predict(asarray([row]))
print('Input: %s, Predicted: %.3f' % (row, yhat[0]))

#-----æ¢¯åº¦æå‡å›å½’
#https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html

from sklearn.ensemble import GradientBoostingRegressor
quantiles = [0.01,0.05,0.50,0.95,0.99] # å››åˆ†ä½æ•°

#-----æ¢¯åº¦æå‡å›å½’å‡½æ•°
def GBM(q): # å…³äºGBRå‡½æ•°ï¼Œè§https://blog.csdn.net/anshiquanshu/article/details/78542852 
    modele = GradientBoostingRegressor(loss='quantile',# lossæŒ‡çš„æ˜¯æ¯ä¸€æ¬¡èŠ‚ç‚¹åˆ†è£‚æ‰€è¦æœ€å°åŒ–çš„æŸå¤±å‡½æ•°
                                        alpha=q, # æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­åˆé€‚çš„å­¦ä¹ ç‡ï¼Œæ›´åˆé€‚çš„å«æ³•â€œæ­¥é•¿â€ï¼Œæ­¥é•¿å†³å®šäº†æ¯ä¸€æ¬¡è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œä¼šå¾€æ¢¯åº¦ä¸‹é™çš„æ–¹å‘ç§»åŠ¨çš„è·ç¦»ï¼Œ
                                                    #å¦‚æœæ­¥é•¿å¾ˆå¤§ï¼Œç®—æ³•ä¼šåœ¨å±€éƒ¨æœ€ä¼˜ç‚¹æ¥å›è·³è·ƒï¼Œä¸ä¼šæ”¶æ•›ï¼Œå¦‚æœæ­¥é•¿å¾ˆå°ï¼Œç®—æ³•æ”¶æ•›é€Ÿåº¦å¾ˆæ…¢ https://www.zhihu.com/question/54097634?sort=created
                                        n_estimators=500, # å®šä¹‰äº†éœ€è¦ä½¿ç”¨åˆ°çš„å†³å®šæ ‘çš„æ•°é‡
                                        max_depth=8,# max_depthå®šä¹‰äº†æ ‘çš„æœ€å¤§æ·±åº¦
                                        learning_rate=.01, # å†³å®šç€æ¯ä¸€ä¸ªå†³å®šæ ‘å¯¹äºæœ€ç»ˆç»“æœçš„å½±å“ã€‚GBMè®¾å®šäº†åˆå§‹çš„æƒé‡å€¼ä¹‹åï¼Œæ¯ä¸€æ¬¡æ ‘åˆ†ç±»éƒ½ä¼šæ›´æ–°è¿™ä¸ªå€¼ï¼Œ
                                                             #è€Œlearning_ rateæ§åˆ¶ç€æ¯æ¬¡æ›´æ–°çš„å¹…åº¦ã€‚ä¸€èˆ¬æ¥è¯´è¿™ä¸ªå€¼ä¸åº”è¯¥è®¾çš„æ¯”è¾ƒå¤§ï¼Œå› ä¸ºè¾ƒå°çš„learning rateä½¿å¾—æ¨¡å‹å¯¹ä¸åŒçš„æ ‘æ›´åŠ ç¨³å¥ï¼Œå°±èƒ½æ›´å¥½åœ°ç»¼åˆå®ƒä»¬çš„ç»“æœã€‚
                                        min_samples_leaf=20, # min_samples_leaf å®šä¹‰äº†æ ‘ä¸­ä¸€ä¸ªèŠ‚ç‚¹æ‰€éœ€è¦ç”¨æ¥åˆ†è£‚çš„æœ€å°‘æ ·æœ¬æ•°
                                        min_samples_split=20) # min_samples_split å®šä¹‰äº†æ ‘ä¸­ç»ˆç‚¹èŠ‚ç‚¹æ‰€éœ€è¦çš„æœ€å°‘çš„æ ·æœ¬æ•° 

    modele.fit (X_train,y_train) # X_train,y_trainå°†å¡«å……æ¨¡å‹
    predict = pd.Series(modele.predict(X_test).round(2)) # roundå–å°æ•°ç‚¹å2ä½

    return predict,modele           

#-----è¿›è¡Œé¢„æµ‹
GBM_models = [] # å°†GBM_modelsçš„å½¢å¼å®šä¹‰ä¸ºåˆ—è¡¨
GBM_actual_pred = pd.DataFrame() # å°†é¢„æµ‹çš„æ¨¡å‹GBM_actual_predå®šä¹‰ä¸ºæ•°æ®æ¡†

for q in quantiles:
    predict,model = GBM(q)
    GBM_models.append(model) # ä¸Šé¢å·²ç»å°†GBM_modelsçš„å½¢å¼å®šä¹‰ä¸ºåˆ—è¡¨ï¼Œè¿™é‡Œå¡«å……åˆ—è¡¨
    GBM_actual_pred = pd.concat([GBM_actual_pred,predict],axis=1) # å¡«å……æ•°æ®æ¡†

GBM_actual_pred.columns = quantiles
GBM_actual_pred['actual'] = y_test
GBM_actual_pred['interval'] = GBM_actual_pred[np.max(quantiles)-np.min(quantiles)]
GBM_actual_pred = GBM_actual_pred.sort_values('interval')
GBM_actual_pred

#-----ç»“æœå¯è§†åŒ–
plt.plot(GBM_actual_pred['actual'], # https://blog.csdn.net/qq_45154565/article/details/109388499
    'go', # g ç»¿è‰²ï¼Œo åœ†åœˆï¼Œgoä»£è¡¨ç»¿è‰²åœ†ç‚¹
    markersize=3,# ç‚¹çš„å°ºå¯¸
    label='Actual')

plt.fill_between(np.arange(GBM_actual_pred.shape[0]),          
    GBM_actual_pred[0.01],
    GBM_actual_pred[0.99],
    alpha=0.5,color="r",
    label="Predicted interval")
          
plt.xlabel("Ordered samples")
plt.ylabel("values and prediction intervals")

plt.xlim([0,100])
plt.ylim([20,60])

plt.legend()
plt.show()

r2 = metrics.r2_score(GBM_actual_pred['actual'],GBM_actual_pred[0.5]).round(2) # è®¡ç®—çº¿æ€§å›å½’å†³å®šç³»æ•°
print('R2 score is {}'.format(r2))

def correctPcnt(actual_pred): # åˆ›å»ºå‡½æ•°æ¥è®¡ç®—æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹
    correct = 0
    for i in range(actual_pred.shape[0]):
        if actual_pred.loc[i,0.01] <= actual_pred.loc[i,'actual'] <= actual_pred.loc[i,0.99]:
            correct += 1
            print (correct/len(y_test))

correctPcnt(GBM_actual_pred)

##éšæœºæ£®æ—----------------------------------------

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators=200,random_state=0,min_samples_split=10)

rf.fit(X_train,y_train)

pred_Q = pd.DataFrame()
for pred in rf.estimators_:
    temp = pd.Series(pred.predict(X_test).tound(2))
    pred_Q = pd.concat([pred_Q,temp],axis=1)
pred_Q.head()

RF_actual_pred = pd.DataFrame()

for q in quantiles:
    s = pred_Q.quantile(q=q,axis = 1)
    RF_actual_pred = pd.concat([RF_actual_pred,s],axis = 1,sort = False)
    
RF_actual_pred.columns = quantiles
RF_actual_pred['actual'] = y_test
RF_actual_pred['interval'] = RF_actual_pred[np.max(quantiles) - np.min(quantiles)]
RF_actual_pred = RF_actual_pred.sort_values('interval')
RF_actual_pred = RF_actual_pred.round(2)
RF_actual_pred

plt.plot(RF_actual_pred['actual'],'go',markersize=3,label='Actual')

plt.fill_between(
    np.arange(RF_actual_pred.shape[0]), RF_actual_pred[0.01], RF_actual_pred[0.99], alpha=0.5, color="r",
    label="Predicted interval")

plt.xlabel("Ordered samples.")
plt.ylabel("Values and prediction intervals.")
plt.xlim([0, 100])
plt.ylim([20, 60])

plt.legend()
plt.show()

#--è®¡ç®—çº¿æ€§å›å½’ç›¸å…³ç³»æ•°
r2 = metrics.r2_score(RF_actual_pred['actual'], RF_actual_pred[0.5]).round(2)
print('R2 score is {}'.format(r2))

correctPcnt(RF_actual_pred)